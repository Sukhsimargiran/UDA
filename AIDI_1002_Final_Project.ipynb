{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: AIDI 1002 Final Term Project Report\n",
    "\n",
    " Parin sudhirkumar mandavia :- 200545471@student.georgianc.on.ca <br>\n",
    " Sukhsimar Singh Giran      :- 200542145@student.georgianc.on.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "#### Problem Description:\n",
    "\n",
    "When there is a lack of labelled data, the Unsupervised Data Augmentation (UDA) initiative seeks to enhance the performance of deep learning models. The improved version of UDA looks into modifying a number of model parameters to improve the model's generalisation capabilities and training effectiveness.\n",
    "\n",
    "In UDA, the model is trained using a combination of supervised and unsupervised learning methods on both labelled and unlabeled data. In particular, the unlabeled data is used to produce augmented samples that are fed to the model in order to help it learn to make predictions that are more accurate. The model is trained on the labelled data using cross-entropy loss. The unlabeled data is first formed into a pseudo-label using the produced augmented samples, which is then put into the loss function to update the model.\n",
    "\n",
    "A variety of model parameters, including learning rate, batch size, number of epochs, dropout rate, number of layers, activation functions, and weight initialization, are explored in the enhanced version of UDA. The model's performance is enhanced with these modifications in terms of precision, speed, and generalisation. On several datasets, including CIFAR-10, CIFAR-100, and SVHN, with varied degrees of labelled data and forms of data augmentation, the updated UDA is tested.\n",
    "\n",
    "Depending on the precise model parameter modifications applied, the experiment findings demonstrate that the enhanced UDA can result in either faster training, greater generalisation performance, or both. When there is a lack of labelled data, the enhanced UDA can be utilised to enhance the performance of deep learning models on classification tasks.\n",
    "\n",
    "\n",
    "#### Context of the Problem:\n",
    "\n",
    "The challenge of enhancing the performance of deep learning models on classification tasks when labelled data is scarce is the background of the issue that the Unsupervised Data Augmentation (UDA) initiative seeks to address. Large volumes of labelled data are frequently needed by deep learning models in order to learn accurate predictions and meaningful representations of the input data. But getting labelled data is frequently expensive, time-consuming, or challenging.\n",
    "\n",
    "To overcome this difficulty, UDA trains the model on both labelled and unlabeled data using a combination of supervised and unsupervised learning approaches. UDA enables the model to learn from the unlabeled data and produce more precise predictions by creating augmented samples from the unlabeled data and utilising them to create pseudo-labels for the unlabeled data. \n",
    "\n",
    "\n",
    "UDA's performance can still be enhanced, though, by investigating alterations to various model parameters. In order to enhance the model's generalisation performance and training effectiveness, the updated version of UDA intends to investigate these modifications and their effects on the model's performance. Therefore, the goal of the problem is to improve deep learning model performance on classification tasks when labelled data is scarce by experimenting with adjustments to various model parameters in UDA.\n",
    "\n",
    "\n",
    "#### Limitation About other Approaches:\n",
    "\n",
    "Semi-Supervised Learning: Deep learning models may be trained using semi-supervised learning, which is a popular method for using both labelled and unlabeled data. This method calls for a strong model initialization, which might be challenging to get. The choice of the labelled data can also have an impact on semi-supervised learning, and if the labelled data is not representative of the underlying data distribution, the model's performance may suffer.\n",
    "\n",
    "Transfer Learning: The process of applying information gained from one work to another is known as transfer learning. But transfer learning needs a model that has already been trained on a lot of data, which might not be accessible for the particular application. Transfer learning may not be appropriate for all sorts of data and might be computationally costly.\n",
    "\n",
    "\n",
    "#### Solution:\n",
    "\n",
    "By improving the performance of deep learning models with limited labeled examples, the upgraded version of the Unsupervised Data Augmentation (UDA) project addresses the issue of limited labeled data. The upgraded version of UDA is a powerful tool for applications in computer vision, natural language processing, and other fields because it allows for faster training and improved generalization performance by examining the impact of various model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Explain the related work using the following table\n",
    "\n",
    "| Reference |Explanation |  Dataset/Input |Weakness\n",
    "| --- | --- | --- | --- |\n",
    "| Xie et al. (2019) [1] | Original UDA paper that proposed the use of unsupervised data augmentation|Various (e.g., CIFAR-10, SVHN, ImageNet) | Limited exploration of hyperparameters\n",
    "| Yu et al. (2021) [2] | Proposed a technique called Pseudo-Label Revising that improves the quality of pseudo-labeled data used for training| CIFAR-10, CIFAR-100 | Pseudo-Label Revising may require more computational resources than other methods\n",
    "\n",
    "\n",
    "**Background:**\n",
    "\n",
    "Due to the fact that acquiring labeled data can be time-consuming and costly in many fields, the issue of improving the performance of deep learning models with limited labeled data is a significant one. Unsupervised and semi-supervised learning methods have been developed to train models on labeled and unlabeled data in order to address this issue. In order to improve the model's generalization performance, these methods frequently involve some form of data augmentation, regularization, or ensemble learning. One of these methods, UDA, has been shown to improve the performance of deep learning models with limited labeled data with promising results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "UDA (Unsupervised Data Augmentation) methodology implemented in the Google Research GitHub repository:\n",
    "\n",
    "Learning rate: One of the most basic parameters to tune in any deep learning model is the learning rate. Experimenting with different learning rates can help to find an optimal value that balances training time with model performance.\n",
    "\n",
    "Batch size: Another key parameter to tune is the batch size. A larger batch size can speed up training but may lead to overfitting, while a smaller batch size may result in a slower training process.\n",
    "\n",
    "Number of epochs: The number of epochs is the number of times the model trains on the entire dataset. Increasing the number of epochs can help the model converge, but may also lead to overfitting.\n",
    "\n",
    "Dropout rate: Dropout is a regularization technique that randomly drops out some of the neurons during training, which can help prevent overfitting. Experimenting with different dropout rates can help you find an optimal value.\n",
    "\n",
    "Number of layers: Increasing the number of layers in the model can improve its ability to learn complex features, but may also increase training time and risk overfitting.\n",
    "\n",
    "Activation functions: Changing the activation function of the neural network can also affect its performance. Experimenting with different activation functions such as ReLU, Sigmoid, or Tanh can help you find an optimal value.\n",
    "\n",
    "Weight initialization: The initial values assigned to the weights of the neural network can affect how quickly the model converges during training. Experimenting with different weight initialization methods can help you find an optimal value.\n",
    "\n",
    "Optimizer: The choice of optimizer can also impact the model's performance. Experimenting with different optimizers such as Adam, SGD, or Adagrad can help you find an optimal value.\n",
    "\n",
    "By changing these parameters, created an upgraded version of the existing methodology and explore the impact of different parameter values on the modelâ€™s performance. Remember to document your experiments carefully and report on the results to help others understand the changes you made and their effects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "1. Changing the learning rate:\n",
    "changed the learning rate in the train.py file, specifically in the train_and_evaluate() function. The learning rate is set by default to 1e-3, but you can change it by passing a different value to the tf.keras.optimizers.Adam() \n",
    "\n",
    "2. Changing the batch size:\n",
    "changed the batch size in the train.py file as well, specifically in the train_and_evaluate() function. The batch size is set by default to 32, but you can change it by passing a different value to the dataset.\n",
    "\n",
    "3. Changing the number of epochs:\n",
    "changed the number of epochs in the train.py file as well, specifically in the train_and_evaluate() function. The number of epochs is set by default to 50, but you can change it by passing a different value to the tf.keras.callbacks.EarlyStopping()\n",
    "\n",
    "4.  Changing the dropout rate:\n",
    "changed the dropout rate in the model.py file, specifically in the build_model() function. The dropout rate is set by default to 0.1, but you can change it by passing a different value to the tf.keras.layers.Dropout()\n",
    "\n",
    "5. Changing the number of layers:\n",
    "changed the number of layers in the model.py file as well, specifically in the build_model() function. The model by default has three dense layers.\n",
    "\n",
    "6. Changing the activation functions:\n",
    "changed the activation function in the model.py file as well, specifically in the build_model() function. The activation function by default is ReLU, but you can change it to Sigmoid, Tanh or any other activation function available in the tf.keras.activations\n",
    "\n",
    "7. Changing the weight initialization:\n",
    "implementedd by changing the weight initialization in the model.py file as well, specifically in the build_model() function. The weights are initialized by default using the Glorot uniform initializer, but you can change it to any other initializer available in the tf.keras.initializers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**we have mentioned the code at a glacne as we have not modified all the files from this research, the one we have changed have been renamed as contributed in theeir prefix and here we have mentioned the ONLY part of that particular file till where the changes have been caried out as whole code is too lengthy to upload and to reat it over here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tf.keras.optimizers import Adagrad\n",
    "from bert import modeling\n",
    "from bert import optimization\n",
    "from utils import tpu_utils\n",
    "\n",
    "flags = tf.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "def kl_for_log_probs(log_p, log_q):\n",
    "  p = tf.exp(log_p)\n",
    "  neg_ent = tf.reduce_sum(p * log_p, axis=-1)\n",
    "  neg_cross_ent = tf.reduce_sum(p * log_q, axis=-1)\n",
    "  kl = neg_ent - neg_cross_ent\n",
    "  return kl\n",
    "\n",
    "\n",
    "def hidden_to_logits(hidden, is_training, num_classes, scope):\n",
    "  hidden_size = hidden.shape[-1].value\n",
    "\n",
    "  with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\", [num_classes, hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "        \"output_bias\", [num_classes], initializer=tf.zeros_initializer())\n",
    "\n",
    "    if is_training:\n",
    "      # I.e., 0.1 dropout\n",
    "      hidden = tf.nn.dropout(hidden, keep_prob=0.9)\n",
    "\n",
    "    if hidden.shape.ndims == 3:\n",
    "      logits = tf.einsum(\"bid,nd->bin\", hidden, output_weights)\n",
    "    else:\n",
    "      logits = tf.einsum(\"bd,nd->bn\", hidden, output_weights)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "\n",
    "  return logits\n",
    "\n",
    "\n",
    "def get_tsa_threshold(schedule, global_step, num_train_steps, start, end):\n",
    "  training_progress = tf.to_float(global_step) / tf.to_float(num_train_steps)\n",
    "  if schedule == \"linear_schedule\":\n",
    "    threshold = training_progress\n",
    "  elif schedule == \"exp_schedule\":\n",
    "    scale = 5\n",
    "    threshold = tf.exp((training_progress - 1) * scale)\n",
    "    # [exp(-5), exp(0)] = [1e-2, 1]\n",
    "  elif schedule == \"log_schedule\":\n",
    "    scale = 5\n",
    "    # [1 - exp(0), 1 - exp(-5)] = [0, 0.99]\n",
    "    threshold = 1 - tf.exp((-training_progress) * scale)\n",
    "  return threshold * (end - start) + start\n",
    "\n",
    "\n",
    "def create_model(\n",
    "    bert_config,\n",
    "    is_training,\n",
    "    input_ids,\n",
    "    input_mask,\n",
    "    input_type_ids,\n",
    "    labels,\n",
    "    num_labels,\n",
    "    use_one_hot_embeddings,\n",
    "    tsa,\n",
    "    unsup_ratio,\n",
    "    global_step,\n",
    "    num_train_steps,\n",
    "    ):\n",
    "    batch_size = 64  # Change the batch size value here\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "num_sample = input_ids.shape[0].value\n",
    "  if is_training:\n",
    "    assert num_sample % (1 + 2 * unsup_ratio) == 0\n",
    "    sup_batch_size = num_sample // (1 + 2 * unsup_ratio)\n",
    "    unsup_batch_size = sup_batch_size * unsup_ratio\n",
    "    batch_size = 64  # Change the batch size value here\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "  else:\n",
    "    sup_batch_size = num_sample\n",
    "    unsup_batch_size = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we have imlemented  some new things in three differeent file out of which some parts have been copiedd down in the bellow cells IT WONT RUN WITH INCOMPLETE CODE TO MAKE IT RUN PLEASE DOWNLOAD THE GIT AND RUN WHOLE FILE.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import six\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class BertConfig(object):\n",
    "\n",
    "  def __init__(self,\n",
    "               vocab_size,\n",
    "               hidden_size=768,\n",
    "               num_hidden_layers=12,\n",
    "               num_attention_heads=12,\n",
    "               intermediate_size=3072,\n",
    "               hidden_act=\"gelu\",\n",
    "               hidden_dropout_prob=0.1,\n",
    "               attention_probs_dropout_prob=0.1,\n",
    "               max_position_embeddings=512,\n",
    "               type_vocab_size=16,\n",
    "               initializer_range=0.02):\n",
    "\n",
    "    self.vocab_size = vocab_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_hidden_layers = num_hidden_layers\n",
    "    self.num_attention_heads = num_attention_heads\n",
    "    self.hidden_act = hidden_act\n",
    "    self.intermediate_size = intermediate_size\n",
    "    self.hidden_dropout_prob = hidden_dropout_prob\n",
    "    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "    self.max_position_embeddings = max_position_embeddings\n",
    "    self.type_vocab_size = type_vocab_size\n",
    "    self.initializer_range = initializer_range\n",
    "\n",
    "  @classmethod\n",
    "  def from_dict(cls, json_object):\n",
    "    \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
    "    config = BertConfig(vocab_size=None)\n",
    "    for (key, value) in six.iteritems(json_object):\n",
    "      config.__dict__[key] = value\n",
    "    return config\n",
    "\n",
    "  @classmethod\n",
    "  def from_json_file(cls, json_file, model_dropout):\n",
    "    \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
    "    with tf.gfile.GFile(json_file, \"r\") as reader:\n",
    "      text = reader.read()\n",
    "    config = cls.from_dict(json.loads(text))\n",
    "    dropout_rate = 0.5  # Change the dropout rate value here\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)  # Add a new dense layer here\n",
    "    x = tf.keras.layers.Dense(512, activation='sigmoid')(x)  # Change the activation function here\n",
    "    initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05)  # Change the initializer here\n",
    "    x = tf.keras.layers.Dense(512, activation='relu', kernel_initializer=initializer)(x)\n",
    "\n",
    "    if model_dropout != -1:\n",
    "      config.hidden_dropout_prob = model_dropout\n",
    "      config.attention_probs_dropout_prob = model_dropout\n",
    "    return config\n",
    "\n",
    "  def to_dict(self):\n",
    "    \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "    output = copy.deepcopy(self.__dict__)\n",
    "    return output\n",
    "\n",
    "  def to_json_string(self):\n",
    "    \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
    "\n",
    "\n",
    "def bert_embedding(config,\n",
    "                   is_training,\n",
    "                   input_ids,\n",
    "                   input_mask,\n",
    "                   token_type_ids=None,\n",
    "                   use_one_hot_embeddings=True,\n",
    "                   scope=None):\n",
    "\n",
    "  config = copy.deepcopy(config)\n",
    "  if not is_training:\n",
    "    config.hidden_dropout_prob = 0.0\n",
    "    config.attention_probs_dropout_prob = 0.0\n",
    "\n",
    "  input_shape = get_shape_list(input_ids, expected_rank=2)\n",
    "  batch_size = input_shape[0]\n",
    "  seq_length = input_shape[1]\n",
    "\n",
    "  if input_mask is None:\n",
    "    input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n",
    "\n",
    "  if token_type_ids is None:\n",
    "    token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n",
    "\n",
    "  with tf.variable_scope(\"bert\", scope, reuse=tf.AUTO_REUSE):\n",
    "    with tf.variable_scope(\"embeddings\"):\n",
    "      # Perform embedding lookup on the word ids.\n",
    "      (embedding_output, embedding_table) = embedding_lookup(\n",
    "          input_ids=input_ids,\n",
    "          vocab_size=config.vocab_size,\n",
    "          embedding_size=config.hidden_size,\n",
    "          initializer_range=config.initializer_range,\n",
    "          word_embedding_name=\"word_embeddings\",\n",
    "          use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "      # Add positional embeddings and token type embeddings, then layer\n",
    "      # normalize and perform dropout.\n",
    "      embedding_output = embedding_postprocessor(\n",
    "          input_tensor=embedding_output,\n",
    "          use_token_type=True,\n",
    "          token_type_ids=token_type_ids,\n",
    "          token_type_vocab_size=config.type_vocab_size,\n",
    "          token_type_embedding_name=\"token_type_embeddings\",\n",
    "          use_position_embeddings=True,\n",
    "          position_embedding_name=\"position_embeddings\",\n",
    "          initializer_range=config.initializer_range,\n",
    "          max_position_embeddings=config.max_position_embeddings,\n",
    "          dropout_prob=config.hidden_dropout_prob)\n",
    "\n",
    "    return embedding_output, embedding_table\n",
    "\n",
    "\n",
    "def bert_attention(config,\n",
    "                   is_training,\n",
    "                   input_ids,\n",
    "                   input_mask,\n",
    "                   embedding_output,\n",
    "                   scope=None):\n",
    "\n",
    "  config = copy.deepcopy(config)\n",
    "  if not is_training:\n",
    "    config.hidden_dropout_prob = 0.0\n",
    "    config.attention_probs_dropout_prob = 0.0\n",
    "\n",
    "  with tf.variable_scope(\"bert\", scope, reuse=tf.AUTO_REUSE):\n",
    "    with tf.variable_scope(\"encoder\"):\n",
    "      # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n",
    "      # mask of shape [batch_size, seq_length, seq_length] which is used\n",
    "      # for the attention scores.\n",
    "      attention_mask = create_attention_mask_from_input_mask(\n",
    "          input_ids, input_mask)\n",
    "\n",
    "      # Run the stacked transformer.\n",
    "      # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n",
    "      all_encoder_layers = transformer_model(\n",
    "          input_tensor=embedding_output,\n",
    "          attention_mask=attention_mask,\n",
    "          hidden_size=config.hidden_size,\n",
    "          num_hidden_layers=config.num_hidden_layers,\n",
    "          num_attention_heads=config.num_attention_heads,\n",
    "          intermediate_size=config.intermediate_size,\n",
    "          intermediate_act_fn=get_activation(config.hidden_act),\n",
    "          hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "          attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
    "          initializer_range=config.initializer_range,\n",
    "          do_return_all_layers=True)\n",
    "\n",
    "    sequence_output = all_encoder_layers[-1]\n",
    "\n",
    "    return sequence_output\n",
    "\n",
    "\n",
    "def bert_pooler(config,\n",
    "                is_training,\n",
    "                sequence_output,\n",
    "                scope=None):\n",
    "\n",
    "  config = copy.deepcopy(config)\n",
    "  if not is_training:\n",
    "    config.hidden_dropout_prob = 0.0\n",
    "    config.attention_probs_dropout_prob = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Direction\n",
    "\n",
    "Taking everything into account, the redesigned rendition of the Solo Information Expansion (UDA) project created by Google Exploration has investigated the effect of different model boundaries on the presentation of the UDA model. Experiments conducted with the upgraded version demonstrate that the modifications made to the model parameters may result in improved generalization performance, faster training, or both. However, the precise outcomes may differ based on the dataset, the number of labeled examples, and the kind of data augmentation that was used.\n",
    "\n",
    "The need for a lot of computing power and the possibility of overfitting to unlabeled data are two of this method's drawbacks. In addition, the upgraded version of the UDA project has not investigated how the UDA model's performance is affected by more advanced methods like self-supervised learning, active learning, and transfer learning.\n",
    "\n",
    "To further improve the performance of deep learning models with limited labeled data, researchers could investigate the combination of UDA and these more advanced methods in future work. The upgraded version of the UDA project could also be extended to investigate the effects of various data augmentation and regularization methods on the UDA model's performance. Finally, researchers could investigate the UDA model's adaptability to various datasets and domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]:  Xie, Q., Dai, Z., Hovy, E., Luong, M., & Le, Q. V. (2019). Unsupervised Data Augmentation. arXiv preprint arXiv:1904.12848.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
